{
  "subreddit": "MachineLearning",
  "url": "https://www.reddit.com/r/MachineLearning",
  "meta": {
    "title": "Machine Learning",
    "members_text": "146,739"
  },
  "posts": [
    {
      "title": "[D] Self-Promotion Thread",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/",
      "score": 6,
      "comments": 2,
      "post_id": "t3_1n67lft",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "AutoModerator",
      "author_id": "t2_6l4z3",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-02T02:15:30.912000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n67lft/d_selfpromotion_thread/",
      "content_preview": "Please post your personal projects, startups, product placements, collaboration needs, blogs etc. Please mention the payment and pricing requirements for products and services. Please do not post link shorteners, link aggregator websites , or auto-subscribe links. -- Any abuse of trust will lead to bans. Encourage others who create new posts for questions to post here instead! Thread will stay alive until next one so keep posting after the date in the title. -- Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_1yz875/styles/profileIcon_klqlly9fc4l41.png?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=4cd002de4de73dc33950158eb385a54026d627e1"
    },
    {
      "title": "[D] NeurIPS is pushing to SACs to reject already accepted papers due to venue constraints",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n4bebi/d_neurips_is_pushing_to_sacs_to_reject_already/",
      "score": 374,
      "comments": 82,
      "post_id": "t3_1n4bebi",
      "post_type": "image",
      "domain": "i.redd.it",
      "author": "impatiens-capensis",
      "author_id": "t2_epo8v0hn1",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-30T20:14:38.046000+0000",
      "content_href": "https://i.redd.it/l46o5xcwr7mf1.png",
      "content_preview": "",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_8r18wx/styles/profileIcon_oa3gvlfsv4xb1.jpeg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=aba9f455a2ddc86d53cc221d12c3aa07d845af76"
    },
    {
      "title": "[R] Graph ML benchmarks and foundation models",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5n1v2/r_graph_ml_benchmarks_and_foundation_models/",
      "score": 25,
      "comments": 1,
      "post_id": "t3_1n5n1v2",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "_puhsu",
      "author_id": "t2_159u6g",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T12:13:29.815000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5n1v2/r_graph_ml_benchmarks_and_foundation_models/",
      "content_preview": "Our team has recently published two graph ML papers: one with a new realistic benchmark and the second one on graph foundation models and how they can be related to tabular foundation models. GraphLand benchmark üìù Paper: https://arxiv.org/abs/2409.14500 üíª Code: https://github.com/yandex-research/graphland It is widely discussed in the community that graph machine learning suffers from the lack of realistic, meaningful, reliable, and diverse benchmarks. We agree with this and we hope that we improve this situation with our recent paper ‚ÄúGraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data‚Äù. GraphLand is a benchmark of 14 diverse graph datasets for node property prediction (both classification and regression) from different industrial applications. The datasets cover realistic machine learning problems and come with rich numerical and categorical node features that are common in real-world applications. Importantly, besides standard random splits, GraphLand provides splits with temporal distributional shifts and the inductive prediction setting, which enable evaluating GNNs in more realistic and challenging scenarios. GraphLand benchmark datasets. We evaluated a wide range of models on GraphLand. This includes several openly available graph foundation models (GFMs), which we found provide very weak performance compared to classical GNNs. Thus, we set out to develop a better GFM, which led us to the next paper... Turning Tabular Foundation Models into Graph Foundation Models üìù Paper: https://arxiv.org/abs/2508.20906 üíª Code: https://github.com/yandex-research/G2T-FM Graphs may come from very different domains and thus may have diverse features varying across datasets. As a result, one of the key challenges for GFMs is how to deal with such diverse heterogeneous features. Prior studies did not fully address this issue, often limiting themselves to text-attributed graphs or relying on simple techniques like PCA and SVD. However, this challenge is not unique to the graph domain. The tabular domain faces exactly the same issue, and recent tabular foundation models like TabPFNv2 successfully deal with it. We‚Äôve decided to transfer their success to graphs. G2T-FM Framework In our framework ‚Äì G2T-FM (Graph-to-Table Foundation Model) ‚Äì we augment the original features with graph information by computing neighborhood feature aggregations and some structure-based encodings, essentially transforming graph tasks to tabular tasks (G2T). After that, we apply TabPFNv2 to these augmented features to get predictions. G2T-FM Results We evaluated G2T-FM on GraphLand and several other graph datasets and found that it shows strong performance in both in-context learning and finetuning settings. In particular, G2T-FM outperforms both well-tuned classic GNNs trained from scratch and prior publicly available GFMs. We hope our work will help develop better GFMs and highlight for the graph community the similarities of graph and tabular domains and the prospects of utilizing tabular foundation models for graph tasks!",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_182taq/styles/profileIcon_nvdyfgkh34yc1.jpeg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=0a1c477654fd64c2ee9a496367c05d57c7c82d7c"
    },
    {
      "title": "[R] Position: The Current AI Conference Model is Unsustainable!",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mo0ynr/r_position_the_current_ai_conference_model_is/",
      "score": 389,
      "comments": 52,
      "post_id": "t3_1mo0ynr",
      "post_type": "gallery",
      "domain": "reddit.com",
      "author": "NuoJohnChen",
      "author_id": "t2_amy8c6cdq",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-12T06:10:51.988000+0000",
      "content_href": "https://www.reddit.com/gallery/1mo0ynr",
      "content_preview": "",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_6.png"
    },
    {
      "title": "[R] Latent Diffusion Question",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5t7cv/r_latent_diffusion_question/",
      "score": 5,
      "comments": 1,
      "post_id": "t3_1n5t7cv",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "AgencyPuzzleheaded",
      "author_id": "t2_6ixbclna",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T16:22:30.753000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5t7cv/r_latent_diffusion_question/",
      "content_preview": "Is this normal for generated data from latent diffusion? The large spikes at the end of the histogram edges. Does this indicate the autoencoder is overfitting? https://preview.redd.it/r-latent-diffusion-question-v0-i1gtm7h3xkmf1.png",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_6.png"
    },
    {
      "title": "[D] Huawei‚Äôs 96GB GPU under $2k ‚Äì what does this mean for inference?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n4y2y3/d_huaweis_96gb_gpu_under_2k_what_does_this_mean/",
      "score": 216,
      "comments": 95,
      "post_id": "t3_1n4y2y3",
      "post_type": "image",
      "domain": "i.redd.it",
      "author": "pmv143",
      "author_id": "t2_teiqn5f1",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-31T15:45:47.217000+0000",
      "content_href": "https://i.redd.it/bnsra3anldmf1.jpeg",
      "content_preview": "",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://preview.redd.it/d-huaweis-96gb-gpu-under-2k-what-does-this-mean-for-v0-bnsra3anldmf1.jpeg?width=640&crop=smart&auto=webp&s=8281a8c8a1e7c20f0f275164df8a879cd131db58"
    },
    {
      "title": "[D] Why aren't there any diffusion speech to text models?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5oznp/d_why_arent_there_any_diffusion_speech_to_text/",
      "score": 5,
      "comments": 12,
      "post_id": "t3_1n5oznp",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "SnappierSoap318",
      "author_id": "t2_16flwj",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T13:40:14.334000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5oznp/d_why_arent_there_any_diffusion_speech_to_text/",
      "content_preview": "Title, I was reading upon diffusion models and speech models and that some of the new diffusion text models are being now developed. Since we know the length of the output that a chunk of audio produces wouldn't it be possible to create a diffusion model to fill in text for the whole length all at once instead of the current auto regressive models? PS: I am really not that advanced so this might be a dumb question.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] How hard is it to get accepted into the AAAI Student Abstract and Poster Program?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5zzln/r_how_hard_is_it_to_get_accepted_into_the_aaai/",
      "score": 0,
      "comments": 1,
      "post_id": "t3_1n5zzln",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "-math-4-life-",
      "author_id": "t2_9f2s8fi",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T20:36:15.382000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5zzln/r_how_hard_is_it_to_get_accepted_into_the_aaai/",
      "content_preview": "Hi everyone, II‚Äôm considering submitting to the AAAI Student Abstract and Poster Program (AAAI-26), but I can‚Äôt find much information about how competitive it is compared to the main technical track. I know the main conference has a pretty low acceptance rate but AAAI doesn‚Äôt seem to share stats for the student program. Has anyone here submitted to or been accepted into this track before? How selective is it? Also, would it be enough if my work is more of an application of existing AI methods to radar (less novelty in the method itself, more novelty in the application)? Or are they mainly looking for new algorithms/AI contributions even in the student track?",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_8u276/styles/profileIcon_b5lfb53kabk61.jpg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=9cbf2d09bcefc09e427c8f2c730660f11a9ca0a6"
    },
    {
      "title": "[R] From Taylor Series to Fourier Synthesis: The Periodic Linear Unit",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mfi8li/r_from_taylor_series_to_fourier_synthesis_the/",
      "score": 229,
      "comments": 53,
      "post_id": "t3_1mfi8li",
      "post_type": "image",
      "domain": "i.redd.it",
      "author": "bill1357",
      "author_id": "t2_14plx8bo",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-02T05:07:31.388000+0000",
      "content_href": "https://i.redd.it/4ppmflqmgjgf1.jpeg",
      "content_preview": "",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_4.png"
    },
    {
      "title": "Recommended Cloud Service [D]",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5mcba/recommended_cloud_service_d/",
      "score": 5,
      "comments": 31,
      "post_id": "t3_1n5mcba",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Fantastic-Nerve-4056",
      "author_id": "t2_13vso8zj9m",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T11:37:07.715000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5mcba/recommended_cloud_service_d/",
      "content_preview": "Hi there, a senior PhD fellow this side. Recently, I entered the LLM space; however, my institute lacks the required computing resources. Hence, my PI suggested that I opt for some cloud services, given that we have a good amount of funding available. So, can anyone recommend a decent cloud platform which, first of all, is budget-friendly, has available A100s, and most importantly, has a friendly UI to run the .ipynb or .py files Any suggestions on it would be appreciated",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[N] Unprecedented number of submissions at AAAI 2026",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/",
      "score": 188,
      "comments": 107,
      "post_id": "t3_1n1wm8n",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "Adventurous-Cut-7077",
      "author_id": "t2_5zen3i6h",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-27T23:27:26.568000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n1wm8n/n_unprecedented_number_of_submissions_at_aaai_2026/",
      "content_preview": "And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast. https://preview.redd.it/n-unprecedented-number-of-submissions-at-aaai-2026-v0-ih3vliracnlf1.png",
      "flair": [
        "News"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_1.png"
    },
    {
      "title": "[P] Beaver: A DSL for Building Streaming ML Pipelines",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5kl6k/p_beaver_a_dsl_for_building_streaming_ml_pipelines/",
      "score": 5,
      "comments": 0,
      "post_id": "t3_1n5kl6k",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Deepblue597",
      "author_id": "t2_f4roshli",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T09:55:59.044000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5kl6k/p_beaver_a_dsl_for_building_streaming_ml_pipelines/",
      "content_preview": "Hi guys! My name is Jason I am an Electrical and Computer Engineering student and for the last year I have been working on my thesis, in which I have developed Beaver¬†‚Äì a domain-specific language (DSL) designed to make building machine learning pipelines for streaming data (e.g., Kafka) much simpler and more accessible. What is Beaver? A DSL that lets you define ML pipelines using a clear, declarative syntax (instead of complex Python code) Generates Python code that integrates with the River library for online ML and supports real-time data streams Includes built-in validation, analysis, and automatic dashboard generation I'm making this post to ask for some feedback. I‚Äôve prepared a user testing experience with 3 tasks (from basic to advanced) that should take about 30-45 minutes. I‚Äôd love to hear your thoughts on usability, clarity, and the overall concept. üìñ Concept overview & docs üìù User testing instructions ü¶´ Example pipeline file üí¨ Feedback form Repo : https://github.com/deepblue597/beaver It is recommended to use the user_testing branch for the feedback. Thank you so much for your time <3",
      "flair": [
        "Project"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] How to do impactful research as a PhD student?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n1gucy/d_how_to_do_impactful_research_as_a_phd_student/",
      "score": 136,
      "comments": 43,
      "post_id": "t3_1n1gucy",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "kekkodigrano",
      "author_id": "t2_6whn0x5u",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-27T13:19:30.545000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n1gucy/d_how_to_do_impactful_research_as_a_phd_student/",
      "content_preview": "Hi everyone, I‚Äôm feeling a bit lost in my PhD journey and would really appreciate some outside perspectives. I‚Äôm doing a PhD on LLMs, and so far I‚Äôve been fairly productive: I‚Äôve published several first-author papers, some accepted at top conferences, others under review with good chances of acceptance. I‚Äôve also had a few successful collaborations. The issue is that I don‚Äôt actually like my research. To be honest, I often feel a bit fraudulent, I rush through projects, produce papers that look solid and well-structured, but in the end, I think their impact is minimal. What I really want is to work on something meaningful and useful. But I keep running into two several obstacles: Any problem I consider tackling already has an overwhelming amount of literature, making it difficult to figure out what truly matters. While I‚Äôm trying to sort this out, there‚Äôs always the risk that someone else publishes a similar idea first, since so many people are working in this space. I work with two supervisors which are both young and highly hambitius. They always propose me new research and collaboration but they never propose me hambitius project or give me time to think deep about something. I'm always involved in fast-paced project that lead to pubblication in few months. Because of this, my current strategy has been to work quickly, run experiments fast, and push out papers, even if they‚Äôre not especially deep or important. I also see publications as my main leverage: since I‚Äôm at a low-ranked university in a unknown group, my publication record feels like the only card I can play to land some opportunities in top labs/companies. At times, I think I just want to land an industry roles as a research engineer, where just having a good numbers of papers on my CV would be enough. But deep down, I do care about my work, and I want to contribute something that feels genuinely important. So I‚Äôm curious: how do you approach doing meaningful research in such a competitive field? How do you balance the pressure to publish with the desire to work on something truly impactful?",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_2.png"
    },
    {
      "title": "[D] AAAI Review Template",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n55mr4/d_aaai_review_template/",
      "score": 11,
      "comments": 2,
      "post_id": "t3_1n55mr4",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "dduka99",
      "author_id": "t2_4k2uoym7",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-31T20:46:47.347000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n55mr4/d_aaai_review_template/",
      "content_preview": "Hello everyone, I‚Äôm serving as a first-time reviewer for AAAI and am getting ready to submit my reviews. I‚Äôm a bit uncertain about the expected structure for the different fields in the review form. For instance, in the ‚ÄúBrief summary of your review‚Äù field, should this be a recap of the paper‚Äôs content or a short explanation of my evaluation and decision? More broadly, I‚Äôd be grateful for any guidance on how to approach the overall submission.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] Dino v3: Self-supervised learning for vision at unprecedented scale",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1ms9d2u/r_dino_v3_selfsupervised_learning_for_vision_at/",
      "score": 214,
      "comments": 17,
      "post_id": "t3_1ms9d2u",
      "post_type": "link",
      "domain": "ai.meta.com",
      "author": "say_wot_again",
      "author_id": "t2_ezakb",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-16T22:07:45.195000+0000",
      "content_href": "https://ai.meta.com/blog/dinov3-self-supervised-vision-model/",
      "content_preview": "",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_3.png"
    },
    {
      "title": "[D] Monthly Who's Hiring and Who wants to be Hired?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/",
      "score": 11,
      "comments": 0,
      "post_id": "t3_1n4jdo7",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "AutoModerator",
      "author_id": "t2_6l4z3",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-31T02:30:34.311000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n4jdo7/d_monthly_whos_hiring_and_who_wants_to_be_hired/",
      "content_preview": "For Job Postings please use this template Hiring: [Location], Salary:[], [Remote | Relocation], [Full Time | Contract | Part Time]    and [Brief overview, what you're looking for] For Those looking for jobs please use this template Want to be Hired: [Location], Salary Expectation:[], [Remote | Relocation], [Full Time | Contract | Part Time]  Resume: [Link to resume] and [Brief overview, what you're looking for] Please remember that this community is geared towards those with experience.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_1yz875/styles/profileIcon_klqlly9fc4l41.png?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=4cd002de4de73dc33950158eb385a54026d627e1"
    },
    {
      "title": "üåüIntroducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinkingüåü [R]",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n4dqsc/introducing_art08b_reasoning_the_way_you_want_it/",
      "score": 8,
      "comments": 0,
      "post_id": "t3_1n4dqsc",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "GuiltyBookkeeper4849",
      "author_id": "t2_zqohbn869",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-30T21:56:20.952000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n4dqsc/introducing_art08b_reasoning_the_way_you_want_it/",
      "content_preview": "Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3- Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts. Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to \"think in rap lyrics\" or \"use bullet points to organize thoughts\" and it will literally reason that way before giving you an answer. You can check out the model on HuggingFace: https://huggingface.co/AGI-0/Art-0-8B (please leave a like in the repo if you like this model) Let me know your thoughts! P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_1.png"
    },
    {
      "title": "[P] Building a YOLOX Plate Detector: Setup, Fine-Tuning, Metrics, Dashcam Inference",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n4asaq/p_building_a_yolox_plate_detector_setup/",
      "score": 2,
      "comments": 0,
      "post_id": "t3_1n4asaq",
      "post_type": "link",
      "domain": "youtube.com",
      "author": "alvises",
      "author_id": "t2_ftyz3",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-30T19:48:34.704000+0000",
      "content_href": "https://www.youtube.com/watch?v=xPJqoX0EYKE",
      "content_preview": "",
      "flair": [
        "Project"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_7.png"
    },
    {
      "title": "[P] Open-Source Protocol designed for Multi-Agent Communication",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n3gfpt/p_opensource_protocol_designed_for_multiagent/",
      "score": 0,
      "comments": 1,
      "post_id": "t3_1n3gfpt",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "Immediate-Cake6519",
      "author_id": "t2_1ahrb9h3c0",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-29T19:13:34.247000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n3gfpt/p_opensource_protocol_designed_for_multiagent/",
      "content_preview": "Project OSS Released MAPLE ‚Äì a Multi Agent Protocol Language Engine designed for fast, secure, and reliable agent communication. ‚Äî a new open-source protocol designed for multi-agent communication at production scale . MAPLE offers features we haven't seen in other protocols: üîß Integrated Resource Management: The ONLY protocol with built-in resource specification, negotiation, and optimization üõ°Ô∏è Link Identification Mechanism (LIM): Revolutionary security through verified communication channels ‚ö° Result<T,E> Type System: ELIMINATES all silent failures and communication errors üåê Distributed State Synchronization: Sophisticated state management across agent networks üè≠ Production-Grade Performance: Very high performance for a feature-rich protocol with sub-millisecond latency üíª pip install maple-oss PyPI here: https://pypi.org/project/maple-oss/ If you‚Äôre building with agents or need robust, real-world communication between systems, check out MAPLE GitHub repo: https://github.com/maheshvaikri-code/maple-oss Please try and test it with your projects. MAPLE Multi Agent Communication Protocol",
      "flair": [
        "Project"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_0.png"
    },
    {
      "title": "[D] Lessons from building an AI data analyst",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5tmp7/d_lessons_from_building_an_ai_data_analyst/",
      "score": 0,
      "comments": 0,
      "post_id": "t3_1n5tmp7",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "pedromnasc",
      "author_id": "t2_nodoh",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T16:38:26.474000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5tmp7/d_lessons_from_building_an_ai_data_analyst/",
      "content_preview": "Hi all, I wrote a post on some lessons from building an AI data analyst: https://pedronasc.com/articles/lessons-building-ai-data-analyst The gap from a nice demo to a real production system is big -> with a lot of yet to be solved challenges. Would love to share ideas with other builders in the space and willing to learn more about it.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_1.png"
    },
    {
      "title": "Finetuning Vision Transformers [D]",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n38fr0/finetuning_vision_transformers_d/",
      "score": 1,
      "comments": 5,
      "post_id": "t3_1n38fr0",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Suitable-Director809",
      "author_id": "t2_4wkwf05y",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-29T14:08:14.502000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n38fr0/finetuning_vision_transformers_d/",
      "content_preview": "Hey, Looking to see how DinoV3 will do on my dataset post finetuning. Any practical advice on finetuning Dino? Scheduler, optimizer, flow - freezing, discriminative lr etc. Any recommandations for blogs or articals related to this?",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_6.png"
    },
    {
      "title": "[R] ŒîAPT: critical review aimed at maximizing clinical outcomes in AI/LLM Psychotherapy",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n0vcrb/r_Œ¥apt_critical_review_aimed_at_maximizing/",
      "score": 116,
      "comments": 5,
      "post_id": "t3_1n0vcrb",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "JustinAngel",
      "author_id": "t2_49klw",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-26T19:28:44.042000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n0vcrb/r_%CE%B4apt_critical_review_aimed_at_maximizing/",
      "content_preview": "Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ https://osf.io/preprints/psyarxiv/4tmde_v1 Since the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. 1. AI therapy research tends to use arbitrary success metrics: the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. \"empathy\" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. 2. AI therapy tools (APTs) are already comparable to human therapists: There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression & anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 & 3.1 talk more about them and put them into clinical context. 3. ŒîAPT allows predicting future clinical outcomes : It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). https://preview.redd.it/r-%CE%B4apt-critical-review-aimed-at-maximizing-clinical-v0-rof96tmbuelf1.png 4. Techniques teaching LLM therapy: Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. https://preview.redd.it/r-%CE%B4apt-critical-review-aimed-at-maximizing-clinical-v0-lbcoovvc0flf1.png 5. Overall, most disadvantages of LLMs are addressable in AI therapy : Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples \"oh wow, hallucinations are going to make AI therapy impossible\". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. 6. video therapy with multi-modal audio/video LLMs: One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio & video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6. Happy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper.",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_edvsc/styles/profileIcon_95t6v9jjzaj01.jpg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=d43123d5fb30842c040f5c4081cf5a319f39b6a0"
    },
    {
      "title": "[D] Upcoming interviews at frontier labs, tips?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n3e27s/d_upcoming_interviews_at_frontier_labs_tips/",
      "score": 99,
      "comments": 16,
      "post_id": "t3_1n3e27s",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "bci-hacker",
      "author_id": "t2_56s4a0u6",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-29T17:42:40.538000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n3e27s/d_upcoming_interviews_at_frontier_labs_tips/",
      "content_preview": "Hi all, I‚Äôm currently interviewing at a few labs for MLE positions and there‚Äôs two interviews in particular that have stumped me that I‚Äôd like some clarity on: Transformer debugging - to my knowledge, the interviewer will provide a buggy implementation of things like causal attention, self-attention, incorrect layer norm, scaling issues, and broadcast/shape mismatch. Is there anything else I‚Äôd need to master here? So far, I‚Äôve only been studying GPT style transformers, should I add BERT to the mix or nah? Training classifier & data analysis. The recruiter said this is around evaluation and model performance. I‚Äôm guessing they‚Äôll throw me an unbalanced dataset and ask me to improve model performance somehow. Things to study here are: 1) chip hguyns book and 2) look at regularization, pandas/sklearn normalization and data clean up methods. How else can I master this topic? Any sample questions you have seen here before? Lastly, what is your go-to source for practicing MLE related topics, both in terms of knowledge-base as well as real interview questions. I tried 1point3acres but very limited when it comes to ML.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] Simple Questions Thread",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5r08b/d_simple_questions_thread/",
      "score": 2,
      "comments": 0,
      "post_id": "t3_1n5r08b",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "AutoModerator",
      "author_id": "t2_6l4z3",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T15:00:11.446000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5r08b/d_simple_questions_thread/",
      "content_preview": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead! Thread will stay alive until next one so keep posting after the date in the title. Thanks to everyone for answering questions in the previous thread!",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_1yz875/styles/profileIcon_klqlly9fc4l41.png?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=4cd002de4de73dc33950158eb385a54026d627e1"
    },
    {
      "title": "[D] Conferences need to find better venues",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/",
      "score": 201,
      "comments": 50,
      "post_id": "t3_1mtfikh",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "AnyIce3007",
      "author_id": "t2_ayem4jqc",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-18T07:40:54.171000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1mtfikh/d_conferences_need_to_find_better_venues/",
      "content_preview": "Better = venues that are virtually accessible for any researcher/author to go to. Just this morning, I'm denied the U.S. B1 visa. I'm supposed to present my work at ICCV 2025 in Hawaii. And during my in-person interview, the Visa Officer did not even bother to ask for the invitation letter. This really blows cause it's supposed to be my first time and I was so excited about attending it. Would love to hear your thoughts about this.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_4.png"
    },
    {
      "title": "[P] Built Sparrow: A custom language model/NLP tool for microcontrollers",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n28w7j/p_built_sparrow_a_custom_language_modelnlp_tool/",
      "score": 6,
      "comments": 2,
      "post_id": "t3_1n28w7j",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "c-f_i",
      "author_id": "t2_1wlucuaxaf",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-28T10:46:50.312000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n28w7j/p_built_sparrow_a_custom_language_modelnlp_tool/",
      "content_preview": "Hey everyone, Don't know if it fully matches this subreddit, but since there have been a lot of discussions around LLMs using a lot of power and water, and even more discussions around LLMs plateauing, as everyone focuses on making the biggest and most powerful model. I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs. The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC. The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2. I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML. It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32. Let me know what you think! Here's a demo video with a ChatGPT simple-webapp to give people something they are familiar with. I'd also like to know opinions around the best way to go forward, release it as a website of sorts, release it as an API like Scikit Learn etc. I have a lot of videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.",
      "flair": [
        "Project"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_7.png"
    },
    {
      "title": "[D] Is modern academic published zero-sum?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1miq2y4/d_is_modern_academic_published_zerosum/",
      "score": 160,
      "comments": 28,
      "post_id": "t3_1miq2y4",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "bigbird1996",
      "author_id": "t2_q0uvm",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-06T00:20:44.167000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1miq2y4/d_is_modern_academic_published_zerosum/",
      "content_preview": "It seems the current state of publishing in A* venues (CVPR, NeurIPS, ICML, ICCV/ECCV) is zero-sum. One person‚Äôs rejection is another person‚Äôs acceptance. Reviewers seem to reject papers just for the sake of rejection. There‚Äôs a sense that some reviewers reject papers not on substantive grounds, but out of an implicit obligation to limit acceptance rates. Rebuttals appear to be pointless as reviewers take stubborn positions and not acknowledge their misunderstandings during this period. Good science just doesn‚Äôt appear to be as valued as the next flashiest LLM/VLM that gets pretty results.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_2.png"
    },
    {
      "title": "[D] OOM When Resuming From Checkpoint",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5qhr4/d_oom_when_resuming_from_checkpoint/",
      "score": 1,
      "comments": 0,
      "post_id": "t3_1n5qhr4",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "New-Skin-5064",
      "author_id": "t2_o4231frz5",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T14:40:06.552000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5qhr4/d_oom_when_resuming_from_checkpoint/",
      "content_preview": "I was training a GPT-2 XL-sized LLM, and I had to stop the run. When I try to resume the run on the same hardware, I get an OOM. I had a similar issue when my model had about 930m parameters, but I solved it by moving all tensors in the model/optimizer state dicts to CPU before saving. When I run this code:optimizer.state = collections.defaultdict(dict)the OOM goes away. The OOM always happens during the optimizer step. I use xm.optimizer_step with the barrier enabled. I have also tried manually sharding the optimizer states using xs.mark_sharding. Here are some details about my project/setup: TPU v3-8 Torch 2.7.0 jax 0.6.2 I use FSDP with SPMD Here is some relevant code from my codebase: Saving: def save_checkpoint(model, optimizer, step, train_device_loader=None):\n    # Save model weights via XLA SPMD checkpoint (supported)\n    os.makedirs(f\"./ckpt-{step}\", exist_ok=True)\n    model_state_dict = model.module.state_dict()\n    for i in model_state_dict.keys():\n        xla_tensor = model_state_dict[i]\n        model_state_dict[i] = xla_tensor.to(\"cpu\")\n        del xla_tensor\n    model_sd = {\"model\": model_state_dict}\n    xm.save(model_sd, f\"./ckpt-{step}/model.pt\")\n\n    # Save host-only states separately (optimizer, step, RNG, dataloader)\n    optim_state = optimizer.state_dict()\n    optim_state_for_saving = {\n        \"state\": {},\n        \"param_groups\": optimizer.state_dict()[\"param_groups\"]\n    }\n    for i in optim_state[\"state\"]:\n        optim_state_for_saving[\"state\"][i] = {}\n        optim_state_for_saving[\"state\"][i][\"step\"] = optim_state[\"state\"][i][\"step\"].to(\"cpu\")\n        optim_state_for_saving[\"state\"][i][\"exp_avg\"] = optim_state[\"state\"][i][\"exp_avg\"].to(\"cpu\")\n        optim_state_for_saving[\"state\"][i][\"exp_avg_sq\"] = optim_state[\"state\"][i][\"exp_avg_sq\"].to(\"cpu\")\n    host_state = {\n        \"optim\": optim_state_for_saving,\n        \"step\": step,\n    }\n\n    if train_device_loader:\n        rng_states = {\n            'torch_rng_state': torch.get_rng_state(),\n            'numpy_rng_state': np.random.get_state(),\n            'random_rng_state': random.getstate(),\n        }\n        dataloader_states = {\n            \"shard_order\": train_device_loader._loader.dataset.shards,\n            \"local_order\": train_device_loader._loader.dataset.curr_order,\n            \"warmup_order\": train_device_loader._loader.dataset.warmup_order,\n            \"warmup_prob\": train_device_loader._loader.dataset.warmup_prob,\n        }\n    else:\n        rng_states = None\n        dataloader_states = None\n\n    # Write host-side files\n    with open(f\"./ckpt-{step}/host_state.pkl\", \"wb\") as f:\n        pickle.dump(host_state, f)\n    if rng_states is not None:\n        with open(f\"./ckpt-{step}/rng.pkl\", \"wb\") as f:\n            pickle.dump(rng_states, f)\n    if dataloader_states is not None:\n        with open(f\"./ckpt-{step}/dataloader.json\", \"w\") as json_file:\n            json.dump(dataloader_states, json_file, indent=4) Loading: if resume_from != \"\":\n        model_sd = torch.load(f\"{resume_from}/model.pt\", map_location='cpu')\n        model.load_state_dict(model_sd[\"model\"])\nmodel = model.to(device)\nif gradient_checkpointing:\n        model = FSDPv2(module=checkpoint_module(model), mesh=mesh)\nelse:\n        model = FSDPv2(module=model, mesh=mesh)\noptimizer = build_optimizer(model, peak_lr, betas, weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*(1-warmup_pct), eta_min=min_lr)\nif resume_from != \"\":\n        xm.mark_step()\n        # 2) Restore host-only states (optimizer, step)\n        with open(f\"{resume_from}/host_state.pkl\", 'rb') as f:\n            host_state = pickle.load(f)\n        optim_state = host_state[\"optim\"]\n        \n        # Load the processed state dict\n        optimizer.load_state_dict(optim_state)\n        del optim_state\n        last_step = host_state[\"step\"]\n        # 3) Restore RNG and dataloader state (if present)\n        try:\n            with open(f\"{resume_from}/rng.pkl\", \"rb\") as f:\n                rng = pickle.load(f)\n            torch.set_rng_state(rng['torch_rng_state'])\n            np.random.set_state(rng['numpy_rng_state'])\n            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])\n        except FileNotFoundError:\n            pass\n        with open(f'{resume_from}/dataloader.json', 'r') as file:\n            dataloader = json.load(file) Step: for k in range(gradient_accumulation_steps):\n    x, y = next(train_iter)\n     with autocast(xm.xla_device(), dtype=torch.bfloat16):\n          loss = model(x, y)\n    (loss / gradient_accumulation_steps).backward()\n     train_loss += loss.detach()\n     xm.mark_step()\n                \ntorch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n                \nxm.optimizer_step(optimizer, barrier=True)\n                \noptimizer.zero_grad()",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] [EMNLP 2025] CCPS: Confidence from Consistency under Perturbation of States ‚Äî Superior Calibration Performance Across Benchmarks/Models",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n2jekd/r_emnlp_2025_ccps_confidence_from_consistency/",
      "score": 1,
      "comments": 2,
      "post_id": "t3_1n2jekd",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "erfan_mhi",
      "author_id": "t2_u669d7x",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-28T17:58:57.895000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n2jekd/r_emnlp_2025_ccps_confidence_from_consistency/",
      "content_preview": "Hi everyone, Our paper ‚Äú Confidence from Consistency under Perturbation of States (CCPS) ‚Äù was accepted to the EMNLP 2025 Main Conference , placing in the top 15% of accepted papers with a final meta-review rating of 9 (strong accept) . üîç Motivation LLMs don‚Äôt just make mistakes, they‚Äôre often confidently wrong. That‚Äôs fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment. ‚ú® What is CCPS? CCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is: If the answer remains stable ‚Üí the model was truly confident. If the answer flips ‚Üí the confidence was unreliable. This approach is simple, efficient, and does not require fine-tuning the base LLM. üìä Results Across LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT): Calibration : Error cut by more than 50%, down to ~4.5% on the toughest benchmarks. Discrimination : More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.). Performance : Boosts accuracy and robustness, all without fine-tuning the base LLM. üí° Why it matters CCPS delivers more reliable, better-calibrated LLMs, models that don‚Äôt just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries. üìé Resources üìÑ Paper: arXiv link üíª Code: GitHub repo üìä Data: HF Dataset Happy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment.",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_fydn1/styles/profileIcon_zo7xqr49rrue1.jpg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=11645b3098c10e31a292788e52721f8dc6fd4616"
    },
    {
      "title": "I built a tool to benchmark tokenizers across 100+ languages and found some wild disparities [R]",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n0r8b7/i_built_a_tool_to_benchmark_tokenizers_across_100/",
      "score": 84,
      "comments": 21,
      "post_id": "t3_1n0r8b7",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "FutureIncrease",
      "author_id": "t2_34af3ox0",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-26T16:54:16.635000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n0r8b7/i_built_a_tool_to_benchmark_tokenizers_across_100/",
      "content_preview": "TL;DR: Created tokka-bench to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks. Links: Live dashboard Full blog post GitHub repo https://preview.redd.it/i-built-a-tool-to-benchmark-tokenizers-across-100-languages-v0-7i03jela9elf1.png The Problem Nobody Talks About I started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps: UTF-8 encoding differences: English: ~1 byte per character Arabic: 2+ bytes per character Chinese: 3+ bytes per character Tokenization bias: Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems. Why This Affects Performance During training: If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder. During inference: Low-resource languages need 2-3x more tokens per sentence: Slower throughput (costs more to serve) Context windows fill up faster More chances to mess up during generation What I Built tokka-bench measures four key things: Efficiency - bytes per token (compression quality) Coverage - unique tokens used (script representation) Word splitting - how often semantic units get fragmented Subword fertility - average tokens per semantic unit Interesting Findings You can actually reverse-engineer training data from tokenizer performance: Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained) Gemma 3: Strong Urdu/Hindi performance gpt-oss: Good Arabic/Gujarati coverage Weirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions. Technical Details Built on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language. Shoutout to Judit √Åcs for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork. PS: if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff. Posted this on LinkedIn/Twitter already but figured r/MachineLearning would appreciate the technical details. Happy to answer questions about methodology or findings!",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_4.png"
    },
    {
      "title": "[R] ‚ÄúHow I‚Äôm structuring a 16M character dialogue corpus for persona reconstruction in LLMs‚Äù",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n2i7iy/r_how_im_structuring_a_16m_character_dialogue/",
      "score": 0,
      "comments": 6,
      "post_id": "t3_1n2i7iy",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Stunning_Put_6077",
      "author_id": "t2_1rymd5326j",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-28T17:14:30.280000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n2i7iy/r_how_im_structuring_a_16m_character_dialogue/",
      "content_preview": "In the past weeks, I‚Äôve been working on a somewhat ‚Äúcrazy‚Äù project: manually splitting and structuring 16 million characters of dialogue data, preparing it for feeding into a model to reconstruct a persona module. Along the way, I‚Äôve noticed a few technical challenges: 1.\tFile size balance Keeping each file around 300k‚Äì400k characters is the most stable. Beyond that, performance tends to drop. 2.\tContext continuity Poor segmentation can easily break the model‚Äôs sense of persona, resulting in inconsistent tone. 3.\tTagging & classification It‚Äôs not just about cutting text, but also annotating emotional states and tonal shifts, so the model can later rebuild ‚Äúmemory‚Äù in a coherent way. This made me realize that large-scale corpus curation is itself a kind of language engineering. It‚Äôs not just data processing ‚Äî it shapes whether an AI can emerge as a whole presence. I‚Äôm curious: In your NLP or LLM practice, how do you balance scale with contextual integrity?",
      "flair": [
        "Research"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] Where to find vast amounts of schemas for AI model training?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n2c588/d_where_to_find_vast_amounts_of_schemas_for_ai/",
      "score": 0,
      "comments": 0,
      "post_id": "t3_1n2c588",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Fragrant-Dog-3706",
      "author_id": "t2_1j0ajy7rie",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-28T13:24:31.300000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n2c588/d_where_to_find_vast_amounts_of_schemas_for_ai/",
      "content_preview": "[D] Looking for massive schema collections for training models working on a project and need to find vast amounts of schemas for training models. specifically looking for financial data (transactions, market data, etc) and retail/ecommerce stuff (product catalogs, user behavior, sales data) but honestly need schemas from pretty much every domain I can get. anyone know where to find quality structured schemas at scale? open to paid sources too. need thousands of different schema types ideally. thanks!",
      "flair": [
        "Research"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n23r3t/r_discrete_diffusion_vla_bringing_discrete/",
      "score": 1,
      "comments": 0,
      "post_id": "t3_1n23r3t",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Lonely-Loquat9638",
      "author_id": "t2_v3d4zpf9",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-28T05:21:15.027000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n23r3t/r_discrete_diffusion_vla_bringing_discrete/",
      "content_preview": "TL;DR. We introduce discrete diffusion as the action decoder inside a single transformer for VLA. Two simple components‚ÄîAdaptive decoding order and Secondary re-masking‚Äîyield consistent action refinement and outperform AR and continuous-diffusion heads. Trains with the same cross-entropy objective as VLMs, preserving pretrained priors. This design shows better success rates vs AR and continuous diffusion. Disclosure: I‚Äôm an author. What‚Äôs new First discrete-diffusion action head for VLA (to our knowledge). Single-transformer, VLM-style training: keeps the discrete token interface and uses the same CE loss as the VLM backbone ‚Üí maximizes retention of pretrained VLM priors . Adaptive decoding order: in each refinement round, we keep easy tokens first via confidence / confidence-gap scores and a cosine keep schedule; the rest remain masked for the next round. Secondary re-masking: previously kept tokens are re-checked (threshold + residual-drop) and re-masked if uncertain/inconsistent, enabling robust cross-round error correction. Why it matters For robotics manipulation tasks, unlike continuous diffusion decoders, our formulation keeps action generation inside a unified transformer and trains with the same cross-entropy objective used by VLMs. This preserves the backbone‚Äôs pretrained vision-and-language capability ‚Äîakin to extending a vocabulary‚Äîwhile opening a path to inherit unified transformers‚Äô scaling behavior , paving the way for large-scale VLA . Moreover, Discrete Diffusion VLA breaks the left-to-right bottleneck of AR decoders: action chunks are adaptively decoded in parallel over a small, fixed number of steps, and uncertain tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement. Links Paper: https://arxiv.org/abs/2508.20072 Demo videos: https://huggingface.co/papers/2508.20072",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_5.png"
    },
    {
      "title": "Arxiv submission on hold  [R]",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n1tdcl/arxiv_submission_on_hold_r/",
      "score": 0,
      "comments": 3,
      "post_id": "t3_1n1tdcl",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "OkOwl6744",
      "author_id": "t2_15111p93rf",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-27T21:14:49.886000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n1tdcl/arxiv_submission_on_hold_r/",
      "content_preview": "Hey Looking for information online about the on hold status but couldn‚Äôt find very clearly. The on hold is automatic or normal? Or if some sort of problem was found ? I already have a DOI from Zenodo, but wanted to publish on arxiv as it seems to be the norm currently. It‚Äôs my first publication there, so I‚Äôm not sure what the process is exactly. Thanks!",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_d5x52i/styles/profileIcon_6i2ombkdgr5f1.png?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=d88d5dff67b8915e9a84b64d16c9830d6ed92cb5"
    },
    {
      "title": "[P] Improving model performance",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5j2sr/p_improving_model_performance/",
      "score": 2,
      "comments": 0,
      "post_id": "t3_1n5j2sr",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Naneet_Aleart_Ok",
      "author_id": "t2_8sl53eko",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-09-01T08:18:25.649000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n5j2sr/p_improving_model_performance/",
      "content_preview": "So I have been working on Continuous Sign Language Recognition (CSLR) for a while. Tried ViViT-Tf, it didn't seem to work. Also, went crazy with it in wrong direction and made an over complicated model but later simplified it to a simple encoder decoder, which didn't work. Then I also tried several other simple encoder-decoder. Tried ViT-Tf, it didn't seem to work. Then tried ViT-LSTM, finally got some results (38.78% word error rate). Then I also tried X3D-LSTM, got 42.52% word error rate. Now I am kinda confused what to do next. I could not think of anything and just decided to make a model similar to SlowFastSign using X3D and LSTM. But I want to know how do people approach a problem and iterate their model to improve model accuracy. I guess there must be a way of analysing things and take decision based on that. I don't want to just blindly throw a bunch of darts and hope for the best.",
      "flair": [
        "Project"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] Computational power needs for Machine Learning/AI",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n1ebmk/r_computational_power_needs_for_machine_learningai/",
      "score": 0,
      "comments": 13,
      "post_id": "t3_1n1ebmk",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Any_Commercial7079",
      "author_id": "t2_hx1syefg",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-27T11:22:40.942000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n1ebmk/r_computational_power_needs_for_machine_learningai/",
      "content_preview": "Hi everyone! As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power. If you work with machine learning (in industry, research, or as a student), I‚Äôd greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure. The survey will take about two to three minutes. Here¬¥s the link: https://survey.sogolytics.com/r/vTe8Sr Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.",
      "flair": [
        "Research"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] Measuring Semantic Novelty in AI Text Generation Using Embedding Distances",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n55r7s/r_measuring_semantic_novelty_in_ai_text/",
      "score": 5,
      "comments": 3,
      "post_id": "t3_1n55r7s",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Outrageous-Travel-80",
      "author_id": "t2_49t8id2f",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-31T20:51:50.639000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n55r7s/r_measuring_semantic_novelty_in_ai_text/",
      "content_preview": "We developed a simple metric to measure semantic novelty in collaborative text generation by computing cosine distances between consecutive sentence embeddings. Key finding: Human contributions showed consistently higher semantic novelty than AI across multiple embedding models (RoBERTa, DistilBERT, MPNet, MiniLM) in our human-AI storytelling dataset. The approach is straightforward - just encode sentences and measure distances between consecutive pairs. Could be useful for evaluating dialogue systems, story generation models, or any sequential text generation task. Some links: Paper site Code Blog post with implementation details The work emerged from studying human-AI collaborative storytelling using improvisational theater techniques (\"Yes! and...\" games).",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_7.png"
    },
    {
      "title": "[D] How do researchers ACTUALLY write code?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mln24c/d_how_do_researchers_actually_write_code/",
      "score": 159,
      "comments": 123,
      "post_id": "t3_1mln24c",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Mocha4040",
      "author_id": "t2_78d53rid",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-09T11:22:53.681000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1mln24c/d_how_do_researchers_actually_write_code/",
      "content_preview": "Hello. I'm trying to advance my machine learning knowledge and do some experiments on my own. Now, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs. It's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch \"debugging\" is good old python print statements. My workflow is the following: have an idea -> check if there is simple hugging face workflow -> docs have changed and/or are incomprehensible how to alter it to my needs -> write simple pytorch model -> get simple data from a dataset -> tokenization fails, let's try again -> size mismatch somewhere, wonder why -> nan values everywhere in training, hmm -> I know, let's ask chatgpt if it can find any obvious mistake -> chatgpt tells me I will revolutionize ai, writes code that doesn't run -> let's ask claude -> claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -> ok, print statements it is -> cuda out of memory -> have a drink. Honestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&b panels, there are for finetuning your training, and that requires training to actually work. Edit: There are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold...",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] I reviewed 100 models over the past 30 days. Here are 5 things I learnt.",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n1p7rb/d_i_reviewed_100_models_over_the_past_30_days/",
      "score": 0,
      "comments": 16,
      "post_id": "t3_1n1p7rb",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "function-devs",
      "author_id": "t2_m8ne9snrf",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-27T18:35:12.732000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n1p7rb/d_i_reviewed_100_models_over_the_past_30_days/",
      "content_preview": "I reviewed 100 models over the past 30 days. Here are 5 things I learnt. TL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer. So I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything. 100 models. 30 days. Too much coffee :( Here's what I found: Model moats are ephemeral Model moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etc My system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation. 2. Open source FTW The gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment. 3.Benchmarks are mostly decieving due to reward hacking Benchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored \"high\" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals. 4.Inference speed is everything Speed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to.. 5.Task-specific models > general purpose models for specialized work. No 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance. Test small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster. What models are in your current prod stack? Any hidden gems I missed in the open source space?",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_3.png"
    },
    {
      "title": "[D] Laptop Suggestion for PhD in ML for Robotics",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n0zndc/d_laptop_suggestion_for_phd_in_ml_for_robotics/",
      "score": 0,
      "comments": 20,
      "post_id": "t3_1n0zndc",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "SwissMountaineer",
      "author_id": "t2_1u7tczelsl",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-26T22:15:24.551000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n0zndc/d_laptop_suggestion_for_phd_in_ml_for_robotics/",
      "content_preview": "Hi! I'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust. My budget is $3000. So far, I have identified the following laptops, but am unsure which would be the best choice. - Razer Blade 16 (either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is ~$850 (with taxes) which is significant. - MSI Vector 16¬†HX¬†AI (RTX‚ÄØ5080, 64‚ÄØGB) - $2600 - ThinkPad P1 Gen 7 (RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations. - Legion Pro 7i Gen10 (RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops. Essentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM? Thank you in advance for any suggestions or feedback! EDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://www.redditstatic.com/avatars/defaults/v2/avatar_default_5.png"
    },
    {
      "title": "[D] Too much of a good thing: how chasing scale is stifling AI innovation",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mzsrt2/d_too_much_of_a_good_thing_how_chasing_scale_is/",
      "score": 14,
      "comments": 26,
      "post_id": "t3_1mzsrt2",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "AntreasAntoniou",
      "author_id": "t2_39jv4u6",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-25T14:58:08.253000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1mzsrt2/d_too_much_of_a_good_thing_how_chasing_scale_is/",
      "content_preview": "Dear r/MachineLearning friends, Hello everyone! I hope you are all doing well out there. I've been observing a pattern in the AI research field that I can only describe as a \"Mass Amnesia.\" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment. In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible. I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;) üîó Full Article: https://pieces.app/blog/the-cost-of-ai-scaling I look forward to your arguments and thoughts. Regards, Antreas PS. This is a repost of https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/ because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on. The post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[P] GPU-based backend deployment for an app",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n00ruv/p_gpubased_backend_deployment_for_an_app/",
      "score": 2,
      "comments": 9,
      "post_id": "t3_1n00ruv",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "feller94",
      "author_id": "t2_2li304vl",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-25T19:54:03.030000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n00ruv/p_gpubased_backend_deployment_for_an_app/",
      "content_preview": "Hi all! I'm drafting an app with pose detection (currently using MediaPipe ) and object detection (early Yolo11 ). Since I cannot run these models on the phone itself, I'm developing the backend separately to be deployed somewhere, to then call it from the app when needed . Basically I would need a GPU-based backend (I can also divide the detections and the actual result usage). Now, I know about HuggingFace of course and I've seen a lot of other hosting platforms, but I wanted to ask if you have any suggestions in this regards? I think I might want to release it as free, or for a one-time low cost (if the costs are too high to support myself), but I also do not know how widespread it can be... You know, either useful and loved or unknown to most. The trick is that, since I would need the APIs always ready to respond, the backend would need to be up and running 24/7 . All of the options seem to be quite costly... Is there any better or worse way to do this?",
      "flair": [
        "Project"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_2vpmx4/styles/profileIcon_odml24pxea981.jpg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=5cf195ca5b82e745c918a170ef4a8968ad5e04f1"
    },
    {
      "title": "[D] - What AI Engineers do in top companies?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1ml76ip/d_what_ai_engineers_do_in_top_companies/",
      "score": 150,
      "comments": 58,
      "post_id": "t3_1ml76ip",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "_crazy_muffin_",
      "author_id": "t2_860l1jn8",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-08T21:10:25.505000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1ml76ip/d_what_ai_engineers_do_in_top_companies/",
      "content_preview": "Joined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work. When I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently. Feel free to answer.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[R] Technical Skills Analysis of Machine Learning Professionals in Canada",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n2rvvh/r_technical_skills_analysis_of_machine_learning/",
      "score": 70,
      "comments": 16,
      "post_id": "t3_1n2rvvh",
      "post_type": "gallery",
      "domain": "reddit.com",
      "author": "eh-tk",
      "author_id": "t2_dtztmwet",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-28T23:37:03.685000+0000",
      "content_href": "https://www.reddit.com/gallery/1n2rvvh",
      "content_preview": "",
      "flair": [
        "Research"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_4vr6si/styles/profileIcon_65blwv1s6q1f1.jpeg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=f4fe9a34606d75253cafaf329d3bde07451d2f43"
    },
    {
      "title": "[D] How did JAX fare in the post transformer world?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mybwih/d_how_did_jax_fare_in_the_post_transformer_world/",
      "score": 149,
      "comments": 72,
      "post_id": "t3_1mybwih",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "TajineMaster159",
      "author_id": "t2_2n76omni",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-23T20:19:36.034000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1mybwih/d_how_did_jax_fare_in_the_post_transformer_world/",
      "content_preview": "A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? Or at least, this is my impression, which I concede might be myopic due to my research and industry needs.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": "https://styles.redditmedia.com/t5_s4mm6/styles/profileIcon_6qti69b8jfjb1.jpg?width=64&height=64&frame=1&auto=webp&crop=64%3A64%2Csmart&s=e65e2eaab07402ffa77e826cf8df2840526a279a"
    },
    {
      "title": "[D] Anyone know how to get Cornell's OpenSurfaces dataset?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1mzsn1q/d_anyone_know_how_to_get_cornells_opensurfaces/",
      "score": 3,
      "comments": 10,
      "post_id": "t3_1mzsn1q",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "Mplus479",
      "author_id": "t2_3vlhfq4v",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-25T14:53:11.409000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1mzsn1q/d_anyone_know_how_to_get_cornells_opensurfaces/",
      "content_preview": "Was it abandoned? The website links are dead.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] What is up with Tensorflow and JAX?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n4nm4h/d_what_is_up_with_tensorflow_and_jax/",
      "score": 70,
      "comments": 29,
      "post_id": "t3_1n4nm4h",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "sourgrammer",
      "author_id": "t2_1i5wwzjuo6",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-31T06:31:14.105000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n4nm4h/d_what_is_up_with_tensorflow_and_jax/",
      "content_preview": "Hi all, been in the Machine Learning world till 2021, I still mostly used the old TF 1.x interface and just used TF2.x for a short time. Last work I did was with CUDA 9. It seems like quite a bit shifted with Tensorflow, I looked at the architecture again to see how much changed. To me, it's incomprehensible. Has Google shifted all efforts towards JAX, a framework with fewer layers than TF?",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] An honest attempt to implement \"Attention is all you need\" paper",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n0d12h/d_an_honest_attempt_to_implement_attention_is_all/",
      "score": 64,
      "comments": 18,
      "post_id": "t3_1n0d12h",
      "post_type": "multi_media",
      "domain": "self.MachineLearning",
      "author": "ZealousidealSalt7133",
      "author_id": "t2_psdtvemga",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-26T05:01:32.040000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n0d12h/d_an_honest_attempt_to_implement_attention_is_all/",
      "content_preview": "I have started working on implementing actual research papers in machine learning and I have started with \"Attention is all you need\" paper. I have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below. https://github.com/MayukhSobo/Transformer Edit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not.",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] How do we make browser-based AI agents more reliable?",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n3g1p7/d_how_do_we_make_browserbased_ai_agents_more/",
      "score": 32,
      "comments": 11,
      "post_id": "t3_1n3g1p7",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "DenOmania",
      "author_id": "t2_75q0xkxf",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-29T18:58:40.309000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n3g1p7/d_how_do_we_make_browserbased_ai_agents_more/",
      "content_preview": "I‚Äôve been experimenting with different approaches for giving AI agents the ability to use browsers in real workflows (data collection, QA automation, multi-step workflows). The promise is huge but the reliability problems are just as big: Sessions break after login or CAPTCHA Agents fail when sites change structure Security is hard to guarantee at scale Each framework has its own dialect / quirks Recently I‚Äôve been looking into managed environments that abstract some of this away. For example, I am using hyperbrowser right now and it does provide a unified layer for running browser-based agents without setting up everything manually. But then my question is... Is there ongoing research or promising directions in making browser-agent interactions more robust? Are there known benchmarks, best practices, or papers that deal with these reliability issues?",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    },
    {
      "title": "[D] Cold start latency for large models: new benchmarks show 141B in ~3.7s",
      "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n01odu/d_cold_start_latency_for_large_models_new/",
      "score": 0,
      "comments": 11,
      "post_id": "t3_1n01odu",
      "post_type": "text",
      "domain": "self.MachineLearning",
      "author": "pmv143",
      "author_id": "t2_teiqn5f1",
      "subreddit_id": "t5_2r3gv",
      "subreddit": "r/MachineLearning",
      "created_ts": "2025-08-25T20:28:04.944000+0000",
      "content_href": "https://www.reddit.com/r/MachineLearning/comments/1n01odu/d_cold_start_latency_for_large_models_new/",
      "content_preview": "Some interesting benchmarks I‚Äôve been digging into: ‚Ä¢~1.3s cold start for a 32B model ‚Ä¢~3.7s cold start for Mixtral-141B (on A100s) ‚Ä¢By comparison, Google Cloud Run reported ~19s for Gemma-3 4B earlier this year, and most infra teams assume 10‚Äì20s+ for 70B+ models (often minutes). If these numbers hold up, it reframes inference as less of an ‚Äúalways-on‚Äù requirement and more of a ‚Äúruntime swap‚Äù problem. Open questions for the community: ‚Ä¢How important is sub-5s cold start latency for scaling inference? ‚Ä¢Would it shift architectures away from dedicating GPUs per model toward more dynamic multi-model serving?",
      "flair": [
        "Discussion"
      ],
      "thumbnail_url": ""
    }
  ],
  "scraped_at": "2025-09-02T03:11:18.368639"
}